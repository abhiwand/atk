# This file contains settings that should be considered global to the system,
# such as connection information for connecting to other systems.

# Configuration for particular plugins should go in the reference.conf of the
# project that provides the plugin.

intel.analytics {

  #Configuration for the IAT metastore
  metastore {

    #Connection information for an in-memory database that gets
    #cleared at startup, only suitable for testing
    connection-h2 {
      # H2 is a in-memory Java database useful for testing
      url = "jdbc:h2:mem:iatest;DB_CLOSE_DELAY=-1"
      driver = "org.h2.Driver"
      username = "" # leave blank, no user or password is needed for H2
      password = "" # leave blank, no user or password is needed for H2
    }

    #Connection information for Postgresql, suitable for normal use
    connection-postgresql {
      host = "invalid-postgresql-host"
      port = 5432
      database = "metastore"
      url = "jdbc:postgresql://"${intel.analytics.metastore.connection-postgresql.host}":"${intel.analytics.metastore.connection-postgresql.port}"/"${intel.analytics.metastore.connection-postgresql.database}
      driver = "org.postgresql.Driver"
      username = "metastore"
      password = "Tribeca123"
    }

    # Choose the connection to use. ${intel.analytics.metastore.connection-postgresql} and
    # ${intel.analytics.metastore.connection-h2} are valid values by default, others can
    # be added in application.conf if desired.
    //connection = ${intel.analytics.metastore.connection-postgresql}
    connection = ${intel.analytics.metastore.connection-h2}

  }

  engine {

    plugin {
      command {
        #List of archives to scan for command plugins
        archives = ["engine-spark", "igiraph-titan", "graphon"]
      }
    }

    default-timeout = 30

    page-size = 1000

    fs {
      # the system will create an "intelanalaytics" folder at this location.
      # All Intel Analytics Toolkit files will be stored somewhere under that base location.
      root = "hdfs://invalid-fsroot-host/user/iauser"
    }

    spark {

      # When master is empty the system defaults to spark://`hostname`:7070 where hostname is calculated from the current system
      master = ""
      # When home is empty the system will check expected locations on the local system and use the first one it finds
      # ("/usr/lib/spark","/opt/cloudera/parcels/CDH/lib/spark/", etc)
      home = ""

      # in cluster mode, set master and home like the example
      // master = "spark://MASTER_HOSTNAME:7077"
      // home = "/opt/cloudera/parcels/CDH/lib/spark"
      // home = "/usr/lib/spark"

      # local mode
      // master = "local[4]"

      # this is the default number of partitions that will be used for RDDs
      default-partitions = 90


      # path to python worker execution, usually to toggle 2.6 and 2.7
      //pythonWorkerExec = "python2.7"
      python-worker-exec = "python"

      conf {
        properties {
          # These key/value pairs will be parsed dynamica2ostlly and provided to SparkConf()
          # See Spark docs for possible values http://spark.apache.org/docs/0.9.0/configuration.html
          # All values should be convertible to Strings

          //spark.akka.frameSize=10000
          //spark.akka.retry.wait=30000
          //spark.akka.timeout=200
          //spark.akka.timeout=30000

          # Memory should be same or lower than what is listed as available in Cloudera Manager
          //spark.executor.memory = "16g"
          spark.executor.memory = "8g"

          //spark.shuffle.consolidateFiles=true

          //spark.storage.blockManagerHeartBeatMs=300000
          //spark.storage.blockManagerSlaveTimeoutMs=300000

          //spark.worker.timeout=600
          //spark.worker.timeout=30000

          //spark.serializer = "org.apache.spark.serializer.KryoSerializer"
          //spark.kryo.registrator = "com.intel.intelanalytics.engine.spark.EngineKryoRegistrator"
        }

      }
    }

    #This section provides overrides to the default Hadoop configuration
    hadoop {
      #The path from which to load base configurations (e.g. core-site.xml would be in this folder)
      configuration.path = "ignore/etc/hadoop/conf"
      mapreduce {
        job.user.classpath.first = true
        framework.name = "yarn"
      }
    }


    giraph = ${intel.analytics.engine.hadoop}
    giraph {
      #Overrides of normal Hadoop settings that are used when running Giraph jobs
      giraph.maxWorkers = 1
      giraph.minWorkers = 1
      giraph.SplitMasterWorker = true
      mapreduce.map.memory.mb = 8192
      mapreduce.map.java.opts = "-Xmx6554m"
      giraph.zkIsExternal = false
      mapred.job.tracker = "not used" #not used - this can be set to anything but 'local' to make Giraph work
      archive.name = "igiraph-titan" #name of the plugin jar (without suffix) to launch
    }

    titan {
      load {
        # documentation for these settings is available on Titan website
        storage {
          backend = "hbase"
          # with clusters the hostname should be a comma separated list of host names with zookeeper role assigned
          hostname = "invalid-titan-host"
          port = "2181"
          batch-loading = "true"
          buffer-size = 2048
          attempt-wait = 300
          lock-wait-time = 400
          lock-retries = 15
          idauthority-retries = 30
          read-attempts = 6
          # Pre-split settngs for large datasets
          // region-count = 100
          // short-cf-names = "true"

        }

        autotype = "none"

        ids {
          block-size = 300000
          renew-timeout = 150000
        }
      }
      query {
        storage {
          # query does use the batch load settings in titan.load
          backend = "hbase"
          hostname = "invalid-titan-query-host"
          port = 2181
        }
        cache {
          # Adjust cache size parameters if you experience OutOfMemory errors during Titan queries
          # Either increase heap allocation for IntelAnalytics Engine, or reduce db-cache-size
          # Reducing db-cache will result in cache misses and increased reads from disk
          db-cache = true
          db-cache-clean-wait = 20
          db-cache-time = 180000
          db-cache-size = 0.3 #Allocates 30% of available heap to Titan (default is 50%)
        }
      }
    }
  }
}