# Archive declaration
intel.analytics.component.archives {
  engine-spark {
    config-path = "intel.analytics.engine-spark"
  }
}

#garbage collector configuration
intel.analytics.engine.gc {
  interval = "30 minutes"
  data-lifespan = "7 days"
}

# Configuration for plugins and other things specific to this archive only
intel.analytics.engine-spark {

  //extra-classpath = [${intel.analytics.engine.hbase.configuration.path}, ${intel.analytics.engine.hadoop.configuration.path}, ${hiveconf}, ${hivelib}]
  extra-classpath = [${intel.analytics.engine.hbase.configuration.path}, ${intel.analytics.engine.hadoop.configuration.path}]


  auto-partitioner {
    # auto-partitioning takes a best guess based on the file size
    file-size-to-partition-size = [{ upper-bound="1MB", partitions = 30 }
                                   { upper-bound="1GB", partitions = 90 },
                                   { upper-bound="5GB", partitions = 200 },
                                   { upper-bound="10GB", partitions = 400 },
                                   { upper-bound="15GB", partitions = 750 },
                                   { upper-bound="25GB", partitions = 1000 },
                                   { upper-bound="50GB", partitions = 1500 },
                                   { upper-bound="100GB", partitions = 2000 },
                                   { upper-bound="200GB", partitions = 3000 },
                                   { upper-bound="300GB", partitions = 4000 },
                                   { upper-bound="400GB", partitions = 5000 },
                                   { upper-bound="600GB", partitions = 7500 }]

    # max-partitions is used if value is above the max upper-bound
    max-partitions = 10000

    repartition {
      # re-partitioning strategies:
      # disabled - disable re-partitioning
      # shrink_only - re-partition only when the number partitions is less than existing partitions. Uses less-expensive Spark merge
      # shrink_or_grow - re-partition can either increase or decrease the number of partitions using more-expensive Spark shuffle
      #                  Using this option will also change the ordering of the frame during the shuffle
      strategy = "shrink_only"

      # percentage change in number of partitions that triggers re-partition
      threshold-percent = 80

      # Used to estimate actual size of the frame for compressed file formats like Parquet.
      # This ratio prevents us from under-estimating the number of partitions for compressed files.
      # compression-ratio=uncompressed-size/compressed-size
      # e.g., compression-ratio=4 if  uncompressed size is 20MB, and compressed size is 5MB
      frame-compression-ratio = 3
    }

    # used by some Spark plugins to set the number of partitions to default-tasks-per-core * number of Spark cores
    default-tasks-per-core = 2

    # use broadcast join if file size is lower than threshold. zero disables broadcast joins.
    # this threshold should be less than the maximum size of results returned to Spark driver (i.e., spark.driver.maxResultSize).
    # to increase Spark driver memory, edit java options (IA_JVM_OPT) in /etc/default/intelanalytics-rest-server
    broadcast-join-threshold = "512MB"
  }
  
  command {
    available = ["graphs.query.gremlin", "graphs.query.histogram", "graphs.query.recommend"]

    #Configuration for plugins defined in this archive begins here

    frames {
      load {
        config {
          # number of rows taken for sample test during frame loading
          schema-validation-sample-rows = 100

          # percentage of maximum rows fail in parsing in sampling test. 50 means up 50% is allowed
          schema-validation-fail-threshold-percentage = 50
        }
      }
    }

    graphs {
      query {
        gremlin {
          class = "com.intel.intelanalytics.engine.spark.graph.query.GremlinQuery"
          config {
            default-timeout = ${intel.analytics.engine.default-timeout}
            titan = ${intel.analytics.engine.titan}
            graphson-mode = "normal" # Valid values: "normal", "compact", "extended"
          }
        }
        histogram {
          class = "com.intel.intelanalytics.engine.spark.graph.query.roc.HistogramQuery"
          config {
            default-timeout = ${intel.analytics.engine.default-timeout}
            titan = ${intel.analytics.engine.titan}
            histogram-buckets = 30
            enable-roc = "false"
            roc-threshold = [0, 0.05, 1]
          }
        }
        recommend {
          class = "com.intel.intelanalytics.engine.spark.graph.query.recommend.RecommendQuery"
          config {
            default-timeout = ${intel.analytics.engine.default-timeout}
            titan = ${intel.analytics.engine.titan}
            vertex_type = "L"
            output_vertex_property_list = "als_result"
            vertex_type_property_key = "vertex_type"
            edge_type_property_key = "splits"
            vector_value = "true"
            bias_on = "false"
            train_str = "TR"
            num_output_results = 10
            left_vertex_name = "user"
            right_vertex_name = "movie"
            left_vertex_id_property_key = "user_id"
            right_vertex_id_property_key = "movie_id"
          }
        }
      }

    }
  }
  queries {
    available = []
  }
}


#included so that conf file can be read during unit tests,
#these will not be used when the application is actually running
intel.analytics.engine {
  default-timeout = 30
  titan {}
}
