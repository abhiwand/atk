==========================
SETTING UP THE ENVIRONMENT
==========================
	Set the environment variables:
		source feateng/feateng-env.sh
	
	Link $PIG_HOME/lib/jython-standalone-2.5.3.jar from $HADOOP_HOME/lib & restart Hadoop
		Go to $HADOOP_HOME/lib and run: ln -s $PIG_HOME/lib/jython-standalone-2.5.3.jar
	
	Install our Python dependencies:
		thrift: https://pypi.python.org/pypi/thrift/0.9.1
		happybase: https://pypi.python.org/pypi/happybase/
		
	Make sure HBase thrift server is running: 
		cd $HBASE_HOME/bin 
	    ./hbase-daemon.sh start thrift -threadpool
	
==========================
RUNNING THE SCRIPTS
==========================
Import some datasets:
	python py-scripts/import.py -i /tmp/sample.log -o log_table -l '\n' -c my_custom_udf.py -s 'f_1:chararray,f_2:double'
	python py-scripts/import_csv.py -i /tmp/test.csv -o dummy_table -s 'f1:chararray,f2:chararray,f3:double,f4:long' -k (-k flag is used to skip the header of the CSV file)
	python py-scripts/import_csv.py -i /tmp/us_states.csv -o us_states -s 'state_name:chararray' -k
	python py-scripts/import_csv.py -i /tmp/test.tsv -o dummy_table -d '\t' -s 'f1:chararray,f2:chararray,f3:double,f4:long' -k # To run a tab separated file specify the delimeter as \t
	python py-scripts/import_json.py -i /tmp/test.json -o test_json_import
	
Then clean the dataset:
	python py-scripts/clean.py -f feature_to_clean -i input_table -o output_table [drops all missing values, to replace them use -r [VALUE | avg]]
	python py-scripts/clean.py  -i dummy_table -o dummy_table_cleaned -s all [drop a record if all features are missing/drop empty records]
	python py-scripts/clean.py  -i dummy_table -o dummy_table_cleaned -s any [drop a record if any of its features is missing/drop empty records]

Then run some transforms:
	python py-scripts/transform.py -i input_table -o input_table_transformed -f feature_to_transform -t EXP -n transformed_feature_name
	python py-scripts/transform.py -i input_table -o input_table_transformed -f feature_to_transform -t LOG10 -n transformed_feature_name
	python py-scripts/transform.py -i input_table -o input_table_transformed -f feature_to_transform -t org.apache.pig.piggybank.evaluation.math.POW -a 2 -n transformed_feature_name
	python py-scripts/transform.py -i input_table -o input_table_transformed -f feature_to_transform -t STND -n transformed_feature_name
	(If you want to also have the original feature (specified with -f) to be written to output, specify an extra -k flag to the transform.py script.)
	
	If you want to update the input table instead of creating a new output table, just specify the same output table with the input table:
	python py-scripts/transform.py -i input_table -o input_table -f feature_to_transform -t LOG10 -n transformed_feature_name -k
	
	Let's apply another transform to a derived feature
	python py-scripts/transform.py -i input_table_transformed -o input_table_multiple_transforms -f feature_to_transform -t EXP -n transformed_feature_name
	
	see -s and -d flags of transform.py which may be useful for showing the available features and derived features in a dataset:
	python py-scripts/transform.py -i input_table_transformed -s
	python py-scripts/transform.py -i input_table -o input_table_transformed -d
	
To run the validation tests:
	cd source_code/feateng
	python feateng/py-scripts/tests/validate_etl.py
		
For a demo of the Python API usage see py-scripts/demo.py [This is experimental, we still work on our API design]

==========================
GB INTEGRATION:
==========================
The gb-integration directory under pig contains the scripts for exploring Pig-GB integration approaches. The gb.params file is used in gb-integration/*.pig scripts for pig-gb integration.

To run the scripts at the top level directory:
1. pig -m pig/gb-integration/gb.params -f pig/gb-integration/gb-mr.pig [Running GB as a MR job]
   --> uses the wiki graph @ /user/user/wiki-input
   [check: hadoop dfs -ls /tmp/gb-wiki-output/edata and hadoop dfs -ls /tmp/gb-wiki-output/vdata]

2. pig -m pig/gb-integration/gb.params -f pig/gb-integration/gb-eval.pig [Running GB as a UDF and writing to a graph db with a custom store function]
   --> uses the input @ /tmp/sample.log

3. pig -m pig/gb-integration/gb.params -f pig/gb-integration/gb-xml-test.pig [A test for showing how to parse XML element by element]
   --> parses the XML file @ /tmp/example.xml 
   
4. To run the gb-mr-hbase.pig script:
   First open a gremlin shell and remove the vertices with name starting with "pig" as our test data will create vertices with name starting with "pig".
	   gremlin> conf = new BaseConfiguration();
	   gremlin> conf.setProperty("storage.backend", "hbase");
	   gremlin> conf.setProperty("storage.hostname", "zt001,zt002,zt003,zt004,zt005,zt006,zt007,zt008,zt009,zt010,zt011,zt012,zt013,zt014,zt015,zt016");
	   gremlin> g = TitanFactory.open(conf);
	   gremlin> g.V().cf_name
	   gremlin> g.V.filter{it.cf_name != null}.filter{it.cf_name.startsWith("pig")}.remove()

   Then upload data to HDFS and start the pig script:
	   hadoop dfs -rmr /tmp/graph_data.csv
	   hadoop dfs -put test-data/graph_data.csv  /tmp/graph_data.csv
	   pig pig/gb-integration/gb-mr-hbase.pig
	   
==========================
OTHER NOTES
==========================
	Tested on Hadoop 1.0.1 and pig-0.12.0
	We depend on datafu-0.0.10.jar (https://github.com/linkedin/datafu). We have already put datafu-0.0.10.jar under feateng/lib
	To get rid of jython logging: export PIG_OPTS="-Dpython.verbose=error"
