# This file contains settings that should be considered global to the system,
# such as connection information for connecting to other systems.

# Configuration for particular plugins should go in the reference.conf of the
# project that provides the plugin.

intel.analytics {

  #Configuration for the IAT metastore
  metastore {

    #Connection information for an in-memory database that gets
    #cleared at startup, only suitable for testing
    connection-h2 {
      # H2 is a in-memory Java database useful for testing
      url = "jdbc:h2:mem:iatest;DB_CLOSE_DELAY=-1"
      driver = "org.h2.Driver"
      username = "" # leave blank, no user or password is needed for H2
      password = "" # leave blank, no user or password is needed for H2
    }

    #Connection information for Postgresql, suitable for normal use
    connection-postgresql {
      host = "invalid-postgresql-host"
      port = 5432
      database = "metastore"
      url = "jdbc:postgresql://"${intel.analytics.metastore.connection-postgresql.host}":"${intel.analytics.metastore.connection-postgresql.port}"/"${intel.analytics.metastore.connection-postgresql.database}
      driver = "org.postgresql.Driver"
      username = "metastore"
      password = "Tribeca123"
    }

    # Choose the connection to use. ${intel.analytics.metastore.connection-postgresql} and
    # ${intel.analytics.metastore.connection-h2} are valid values by default, others can
    # be added in application.conf if desired.
    connection = ${intel.analytics.metastore.connection-postgresql}
  }

  engine {

    plugin {
      command {
        #List of archives to scan for command plugins
        archives = ["engine-spark", "igiraph-titan", "graphon"]
      }
    }

    default-timeout = 30

    page-size = 1000

    fs {
      # the system will create an "intelanalaytics" folder at this location.
      # All Intel Analytics Toolkit files will be stored somewhere under that base location.
      root = "hdfs://invalid-fsroot-host/user/iauser"
    }

    spark {

      # When master is empty the system defaults to spark://`hostname`:7070 where hostname is calculated from the current system
      master = ""
      # When home is empty the system will check expected locations on the local system and use the first one it finds
      # ("/usr/lib/spark","/opt/cloudera/parcels/CDH/lib/spark/", etc)
      home = ""

      # in cluster mode, set master and home like the example
      // master = "spark://MASTER_HOSTNAME:7077"
      // home = "/opt/cloudera/parcels/CDH/lib/spark"
      // home = "/usr/lib/spark"

      # local mode
      // master = "local[4]"

      # path to python worker execution, usually to toggle 2.6 and 2.7
      python-worker-exec = "python2.7" 

      conf {
        properties {
          # These key/value pairs will be parsed dynamica2ostlly and provided to SparkConf()
          # See Spark docs for possible values http://spark.apache.org/docs/0.9.0/configuration.html
          # All values should be convertible to Strings

          //spark.akka.frameSize=10000
          //spark.akka.retry.wait=30000
          //spark.akka.timeout=200
          //spark.akka.timeout=30000

          # Memory should be same or lower than what is listed as available in Cloudera Manager
          //spark.executor.memory = "16g"
          spark.executor.memory = "8g"
          spark.hadoop.validateOutputSpecs = false

          //spark.shuffle.consolidateFiles=true

          //spark.storage.blockManagerHeartBeatMs=300000
          //spark.storage.blockManagerSlaveTimeoutMs=300000

          //spark.worker.timeout=600
          //spark.worker.timeout=30000

          // kryo fails for some plugins (e.g. ClassificationMetrics) on clusters, temporarily turning it off, we'll have to re-enable on a per-plugin basis
          //spark.serializer = "org.apache.spark.serializer.KryoSerializer"
          //spark.kryo.registrator = "com.intel.intelanalytics.engine.spark.EngineKryoRegistrator"
        }

      }
    }

    #This section provides overrides to the default Hadoop configuration
    hadoop {
      #The path from which to load base configurations (e.g. core-site.xml would be in this folder)
      configuration.path = "ignore/etc/hadoop/conf"
      mapreduce {
        job.user.classpath.first = true
        framework.name = "yarn"
      }
    }


    giraph = ${intel.analytics.engine.hadoop}
    giraph {
      #Overrides of normal Hadoop settings that are used when running Giraph jobs
      giraph.maxWorkers = 1
      giraph.minWorkers = 1
      giraph.SplitMasterWorker = true
      //mapreduce.map.memory.mb = 8192
      //mapreduce.map.java.opts = "-Xmx6554m"
      giraph.zkIsExternal = false
      mapred.job.tracker = "not used" #not used - this can be set to anything but 'local' to make Giraph work
      archive.name = "igiraph-titan" #name of the plugin jar (without suffix) to launch
    }

    titan {
      load {
        # documentation for these settings is available on Titan website
        storage {
          backend = "hbase"
          # with clusters the hostname should be a comma separated list of host names with zookeeper role assigned
          hostname = "invalid-titan-host"
          port = "2181"

          # Whether to enable batch loading into the storage backend. Set to true for bulk loads.
          batch-loading = true

          # Size of the batch in which mutations are persisted
          buffer-size = 2048

          lock {
            #Number of milliseconds the system waits for a lock application to be acknowledged by the storage backend
            wait-time = 400

            #Number of times the system attempts to acquire a lock before giving up and throwing an exception
            retries = 15
          }

          hbase {
            # Pre-split settngs for large datasets
            // region-count = 100
            short-cf-names = true
            compression-algorithm = "SNAPPY"
          }

          cassandra {

          }

        }

        autotype = "none"

        ids {
          #Globally reserve graph element IDs in chunks of this size. Setting this too low will make commits
          #frequently block on slow reservation requests. Setting it too high will result in IDs wasted when a
          #graph instance shuts down with reserved but mostly-unused blocks.
          block-size = 300000

          #Number of partition block to allocate for placement of vertices
          num-partitions = 30

          #The number of milliseconds that the Titan id pool manager will wait before giving up on allocating a new block of ids
          renew-timeout = 150000

          #When true, vertices and edges are assigned IDs immediately upon creation. When false, IDs are assigned
          #only when the transaction commits. Must be disabled for graph partitioning to work.
          flush = true

          authority {
            #This setting helps separate Titan instances sharing a single graph storage
            #backend avoid contention when reserving ID blocks, increasing overall throughput.
            #GLOBAL_AUTO = Titan randomly selects a tag from the space of all possible tags when performing allocations.
            conflict-avoidance-mode = "GLOBAL_AUTO"

            #The number of milliseconds the system waits for an ID block reservation to be acknowledged by the storage backend
            wait-time = 300

            # Number of times the system attempts ID block reservations with random conflict avoidance tags
            # before giving up and throwing an exception
            randomized-conflict-avoidance-retries = 10
          }
        }
      }

      query {
        storage {
          # query does use the batch load settings in titan.load
          backend = "hbase"
          hostname = "invalid-titan-query-host"
          port = 2181
        }
        cache {
          # Adjust cache size parameters if you experience OutOfMemory errors during Titan queries
          # Either increase heap allocation for IntelAnalytics Engine, or reduce db-cache-size
          # Reducing db-cache will result in cache misses and increased reads from disk
          db-cache = true
          db-cache-clean-wait = 20
          db-cache-time = 180000
          db-cache-size = 0.3 #Allocates 30% of available heap to Titan (default is 50%)
        }
      }
    }
  }
}
