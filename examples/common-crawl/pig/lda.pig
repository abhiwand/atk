register target/IntelGraphAnalytics-0.0.1-SNAPSHOT-jar-with-dependencies.jar                
DEFINE ArcLoader com.intel.graph.analytics.examples.ArcLoader();
DEFINE ExtractText com.intel.graph.analytics.examples.ExtractText();
DEFINE RemoveSymbols com.intel.graph.analytics.examples.RemoveSymbols();
DEFINE RemoveNums com.intel.graph.analytics.examples.RemoveNums();
DEFINE RowKeyAssignerUDF com.intel.graph.analytics.examples.RowKeyAssignerUDF();


raw_data = LOAD '/user/mohitdee/arc_data/100.arc.gz' USING ArcLoader() AS (url:chararray, timestamp:chararray, content:chararray);
url_text = FOREACH raw_data GENERATE url AS url, FLATTEN(ExtractText(content)) AS text;

lower_flatten = FOREACH url_text GENERATE url, LOWER(text) as lower_clean_text: chararray;
--limited = LIMIT lower_flatten 2;

clean_symbols = FOREACH lower_flatten GENERATE url, RemoveSymbols(lower_clean_text) as cleaned_text:chararray;
clean_numerics = FOREACH clean_symbols GENERATE url, RemoveNums(cleaned_text) as cleaned_text:chararray;

splitted_text = FOREACH clean_numerics GENERATE url,'d' as identifier, FLATTEN(TOKENIZE(cleaned_text)) as tokens;
-- remove common words from analysis
stop_words = LOAD '/user/user/stop_words.txt' USING PigStorage('\n') AS (words:chararray);
exclude = JOIN splitted_text By tokens LEFT OUTER, stop_words BY words USING 'replicated';
remove = filter exclude by words IS NULL;
flatten_remove = foreach remove generate url,identifier, tokens; 
-- remove words based on word length
filter_small_words = FILTER  flatten_remove BY SIZE(tokens) >4;
--giraph requires edges to be bidirectional
flip_edges = FOREACH filter_small_words GENERATE tokens as url,'w' as identifier, url as tokens;
unioned = UNION filter_small_words,flip_edges;
generated = FOREACH unioned GENERATE url as id1, identifier as identifier, tokens as id2;

grouped = GROUP generated BY (id1,identifier,id2);      
--graph = FOREACH  grouped GENERATE group , deduped.tokens;
graph = FOREACH grouped GENERATE group.url,group.identifier, group.tokens, COUNT(generated) AS wordCount;



-- things starts getting murkier
-- writing to titan
with_hbase_keys = FOREACH graph GENERATE RowKeyAssignerUDF(*);

final_graph = FOREACH with_hbase_keys GENERATE FLATTEN($0) 
			  AS (key:chararray, src_domain:chararray, dest_domain:chararray, num_links:long);

DEFINE STORE_GRAPH(edge_list, hbase_out_table, graph_db) RETURNS void {
  stored_graph = MAPREDUCE 'graphbuilder-2.0-hadoop-job.jar' STORE final_graph INTO 'hbase://pagerank_edge_list' USING org.apache.pig.backend.hadoop.hbase.HBaseStorage('cf:src_domain cf:dest_domain cf:num_links') LOAD '/tmp/empty' using TextLoader() as (line:chararray) `com.intel.hadoop.graphbuilder.demoapps.tabletographdb.TableToGraphDB -conf hbaseToTitan.xml -t pagerank_edge_list -v "cf:src_domain" "cf:dest_domain" -e "cf:src_domain,cf:dest_domain,link,cf:num_links"`;
  STORE stored_graph INTO '/tmp/tmp_store';
};
STORE_GRAPH(final_graph, 'hbase://pagerank_edge_list', 'Titan');
--final = FOREACH counts GENERATE flatten(wordDoc) , wordCount;     


