==========================
SETTING UP THE ENVIRONMENT
==========================
	Set the environment variables:
		export FEATURE_ENG_HOME=/home/user/nezih/workspace/feature_engineering
		export PIG_HOME=/home/user/pig-0.12.0
		export PATH=$PIG_HOME/bin:$PATH
		export JYTHONPATH=$FEATURE_ENG_HOME/py-scripts/                         
		(REQUIRED TO SHIP THE argparse_lib.py module to the worker nodes)
	
	Link $PIG_HOME/lib/jython-standalone-2.5.3.jar from $HADOOP_HOME/lib & restart Hadoop
		Go to $HADOOP_HOME/lib and run: ln -s $PIG_HOME/lib/jython-standalone-2.5.3.jar
	
	Install our Python dependencies:
		thrift: https://pypi.python.org/pypi/thrift/0.9.1
		happybase: https://pypi.python.org/pypi/happybase/
		
	Make sure HBase thrift server is running: 
		cd $HBASE_HOME/bin 
	    ./hbase-daemon.sh start thrift -threadpool
	
==========================
RUNNING THE SCRIPTS
==========================
Import some datasets:
	python py-scripts/import.py -i /tmp/sample_shaw.log -o shaw_table -l '\n' -c custom-parsers/shaw_udfs.py -s 'timestamp:chararray,event_type:chararray,method:chararray,duration:double,item_id:chararray,src_tms:chararray,dst_tms:chararray'
	python py-scripts/import_csv.py -i /tmp/test.csv -o dummy_table -s 'f1:chararray,f2:chararray,f3:double,f4:long' (If your CSV contains a header line as the first line you can skip loading that line with the -k flag)
	python py-scripts/import_csv.py -i /tmp/us_states.csv -o us_states -s 'state_name:chararray' -k
	python py-scripts/import_json.py -i /tmp/test.json -o test_json_import
	
Then clean the dataset:
w	python py-scripts/clean.py -f duration -i shaw_table -o shaw_cleaned [drops all missing values, to replace them use -r [VALUE | avg]] 

Then run some transforms:
	python py-scripts/transform.py -i shaw_table -o shaw_table_transformed -f duration -t EXP -n exp_duration
	python py-scripts/transform.py -i shaw_table -o shaw_table_transformed -f duration -t LOG10 -n log10_duration
	python py-scripts/transform.py -i shaw_table -o shaw_table_transformed -f duration -t org.apache.pig.piggybank.evaluation.math.POW -a 2 -n duration_squared
	python py-scripts/transform.py -i shaw_table -o shaw_table_transformed -f duration -t STND -n normalized_duration
	(If you want to also have the original feature (specified with -f) to be written to output, specify an extra -k flag to the transform.py script.)
	
	If you want to update the input table instead of creating a new output table, just specify the same output table with the input table:
	python py-scripts/transform.py -i shaw_table -o shaw_table -f duration -t LOG10 -n log10_duration -k
	
	Let's apply another transform to a derived feature
	python py-scripts/transform.py -i shaw_table_transformed -o shaw_table_multiple_transforms -f log10_duration -t EXP -n some_other_feature
	
	see -s and -d flags of transform.py which may be useful for showing the available features and derived features in a dataset:
	python py-scripts/transform.py -i shaw_table_transformed -s
	python py-scripts/transform.py -i shaw_table -o shaw_table_transformed -d
	
Validate the transformation result:
	First run a transformation with -k to keep the original feature in the output table:
		python py-scripts/transform.py -i shaw_table -o shaw_table_transformed -f duration -t LOG10 -n log10_duration -k
	Then validate the output:
		python py-scripts/tests/validate_transform.py -f duration -i shaw_table_transformed -t LOG10 -n log10_duration
	Similarly for other transformations.
		
For a demo of the Python API usage see py-scripts/demo.py [This is experimental, we still work on our API design]

==========================
GB INTEGRATION:
==========================
The gb-integration directory under pig contains the scripts for exploring Pig-GB integration approaches. The gb.params file is used in gb-integration/*.pig scripts for pig-gb integration.

To run the scripts at the top level directory:
1. pig -m pig/gb-integration/gb.params -f pig/gb-integration/gb-mr.pig [Running GB as a MR job]
   --> uses the wiki graph @ /user/user/wiki-input
   [check: hadoop dfs -ls /tmp/gb-wiki-output/edata and hadoop dfs -ls /tmp/gb-wiki-output/vdata]

2. pig -m pig/gb-integration/gb.params -f pig/gb-integration/gb-eval.pig [Running GB as a UDF and writing to a graph db with a custom store function]
   --> uses the input @ /tmp/sample.log

3. pig -m pig/gb-integration/gb.params -f pig/gb-integration/gb-xml-test.pig [A test for showing how to parse XML element by element]
   --> parses the XML file @ /tmp/example.xml 
   
4. To run the gb-mr-hbase.pig script:
   First open a gremlin shell and remove the vertices with name starting with "pig" as our test data will create vertices with name starting with "pig".
	   gremlin> conf = new BaseConfiguration();
	   gremlin> conf.setProperty("storage.backend", "hbase");
	   gremlin> conf.setProperty("storage.hostname", "zt001,zt002,zt003,zt004,zt005,zt006,zt007,zt008,zt009,zt010,zt011,zt012,zt013,zt014,zt015,zt016");
	   gremlin> g = TitanFactory.open(conf);
	   gremlin> g.V().cf_name
	   gremlin> g.V.filter{it.cf_name != null}.filter{it.cf_name.startsWith("pig")}.remove()

   Then upload data to HDFS and start the pig script:
	   hadoop dfs -rmr /tmp/graph_data.csv
	   hadoop dfs -put test-data/graph_data.csv  /tmp/graph_data.csv
	   pig pig/gb-integration/gb-mr-hbase.pig
	   
==========================
OTHER NOTES
==========================
	Tested on Hadoop 1.0.1 and pig-0.12.0
	We depend on datafu-0.0.10.jar (https://github.com/linkedin/datafu). We have already put datafu-0.0.10.jar under feature_engineering/lib
	If you run 'mvn clean package' directly TestErrorHandling.testErroneousJavaUDF() will fail since that test case registers the 'target/TRIB-FeatureEngineering-0.0.1-SNAPSHOT.jar'
	in the tested pig script. So you need to first skip tests during mvn packaging with 'mvn clean package -DskipTests -P tribeca-one', and then run the tests with 'mvn test -P tribeca-one -Dsurefire.useFile=false'.
	To run individual tests: mvn -Dtest=TestShawETL test
	To show stack traces use -Dsurefire.useFile=false 
	To get rid of jython logging: export PIG_OPTS="-Dpython.verbose=error"