intel.analytics.engine {

  max-rows = 20

  hadoop {
    mapreduce {
      jobtracker.address = ip-172-31-43-19
    }
  }

  spark {
    conf {
      properties {
        // These key/value pairs will be parsed dynamically and be provided to SparkConf()
        // See Spark docs for possible values http://spark.apache.org/docs/0.9.0/configuration.html
        // All values should be convertible to Strings
        //spark.executor.memory = "16g"
      }
    }
  }

  algorithm {
    als {
      giraph {
        split-master-worker = true
        mapper-memory = 8192
        mapper-heap = "-Xmx7g"
        zookeeper-external = false
      }
      max-supersteps = 10
      convergence-threshold = 0
      lambda = 0.065
      feature-dimension = 3
      bias-on = true
      learning-curve-output-interval = 1
      max-val = Infinity
      min-val = "-Infinity"
      bidirectional-check = true
    }
  }

  query {
    ALSQuery {
      key-name = "id"
      vertex-type-name = "vertex_type"
      bias-on = true
      edge-type-name = "edge_type"
      edge-type = "edge"
      feature-dimensions = 1
      left-type = "L"
      right-type = "R"
      left-name = "user"
      right-name = "item"
      result-property-list = "als_p0;als_p1;als_p3;als_bias"
      train = "TR"
    }
  }

  graphbuilder {
    load {
      // documentation for these settings is available on Titan website
      storage {
        backend = "hbase"
        // with clusters the hostname should be a comma separated list of host names with zookeeper role assigned
        hostname = "localhost"
        batch-loading = "true"
        buffer-size = 2048
        attempt-wait = 300
        lock-wait-time = 400
        lock-retries = 15
        idauthority-retries = 30
        read-attempts = 6
      }

      autotype = "none"

      ids {
        block-size = 300000
        renew-timeout = 150000
      }
    }
  }

  titan {
    load {
      // documentation for these settings is available on Titan website
      storage {
        backend = "hbase"
        // with clusters the hostname should be a comma separated list of host names with zookeeper role assigned
        hostname = "localhost"
        batch-loading = "true"
        buffer-size = 2048
        attempt-wait = 300
        lock-wait-time = 400
        lock-retries = 15
        idauthority-retries = 30
        read-attempts = 6
      }

      autotype = "none"

      ids {
        block-size = 300000
        renew-timeout = 150000
      }
    }
  }
}