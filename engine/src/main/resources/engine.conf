engine {
    hadoop {
        mapreduce {
            jobtracker.address = ip-172-31-43-19
        }
    }

    algorithm {
        als {
            giraph {
                split-master-worker          = true
                mapper-memory                = 8192
                mapper-heap                  = "-Xmx7g"
                zookeeper-external           = false
            }
            max-supersteps                   = 10
            convergence-threshold            = 0
            lambda                           = 0.065
            feature-dimension                = 3
            bias-on                          = true
            learning-curve-output-interval   = 1
            max-val                          = Infinity
            min-val                          = "-Infinity"
            bidirectional-check              = true
        }
    }

    query {
        ALSQuery {
            key-name                = "id"
            vertex-type-name        = "vertex_type"
            bias-on                 = true
            edge-type-name          = "edge_type"
            edge-type               = "edge"
            feature-dimensions      = 1
            left-type               = "L"
            right-type              = "R"
            left-name               = "user"
            right-name              = "item"
            result-property-list    = "als_p0;als_p1;als_p3;als_bias"
            train                   = "TR"
        }
    }

    graphbuilder {
        load {
            storage {
                backend         =  "hbase"
                hostname        =  "localhost"
                batch-loading   =  "true"
                buffer-size     =  2048
                attempt-wait    =  300
                lock-wait-time  =  400
                lock-retries    =  15
                idauthority-retries = 30
                read-attempts   = 6
            }

            autotype                =  "none"

            ids {
                block-size          = 300000
                renew-timeout       = 150000
            }
        }
    }

    titan {
        load {
            storage {
                backend         =  "hbase"
                hostname        =  "localhost"
                batch-loading   =  "true"
                buffer-size     =  2048
                attempt-wait    =  300
                lock-wait-time  =  400
                lock-retries    =  15
                idauthority-retries = 30
                read-attempts   = 6
            }

            autotype                =  "none"

            ids {
                block-size          = 300000
                renew-timeout       = 150000
            }
        }
    }
}